{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Mottainai docs Build powerful, flexible and decentralized pipelines playable locally. Manage, Publish and release your task's produced content. Mottainai is a Task Distributed, Continous Integration and Delivery system - it allows you to build, test, deploy and manage content built from custom tasks from different nodes in a network. You can hook specific tasks to Git repositories in a CI style, or either directly execute pipelines or production tasks in safe environments. It supports different brokering backends: AMQP, Redis, Memcache, AWS SQS, DynamoDB, Google Pub/Sub and MongoDB. It was developed for building and releasing packages for Linux Distributions, it is used by Sabayon Linux to produce and orchestrate builds of community repositories and to build LiveCDs - but it's suitable for every workflow which is artefact-oriented. For more, see also the Official Gentoo project page . Warning : This is still alpha software - API is not stable and subject to constant changes still. Getting started First steps to start to build! How it works? Setup Tasks and pipelines Plans Powerful CLI to the rescue!","title":"Home"},{"location":"#welcome-to-mottainai-docs","text":"Build powerful, flexible and decentralized pipelines playable locally. Manage, Publish and release your task's produced content. Mottainai is a Task Distributed, Continous Integration and Delivery system - it allows you to build, test, deploy and manage content built from custom tasks from different nodes in a network. You can hook specific tasks to Git repositories in a CI style, or either directly execute pipelines or production tasks in safe environments. It supports different brokering backends: AMQP, Redis, Memcache, AWS SQS, DynamoDB, Google Pub/Sub and MongoDB. It was developed for building and releasing packages for Linux Distributions, it is used by Sabayon Linux to produce and orchestrate builds of community repositories and to build LiveCDs - but it's suitable for every workflow which is artefact-oriented. For more, see also the Official Gentoo project page . Warning : This is still alpha software - API is not stable and subject to constant changes still.","title":"Welcome to Mottainai docs"},{"location":"#getting-started","text":"First steps to start to build! How it works? Setup Tasks and pipelines Plans Powerful CLI to the rescue!","title":"Getting started"},{"location":"general/","text":"General concepts Mottainai is composed of 4 main components: Mottainai Server - central node to dispatch tasks and collect artefacts from build Mottainai CLI - Application to interface with Mottainai Server Replicant - CLI Application to manage an infrastructure state Mottainai Agent - Runtime for nodes that are executing tasks Mottainai Bridge - Suite of library/binaries to listen on events emitted by the infrastructure Server The Server is the main compontent which orchestrate builds across nodes: it dispatches tasks to the nodes using a broker implementation and it take cares of handling the artefacts from the build ( any kind of produced content from your build ). You can customize more the infrastructure behind it, and setup Ceph or other storage engines for permanent and distributed storage. Agent The Agent is the main software which is executing the tasks, is being run by nodes belonging to the cluster. Among its duties, it takes care of executing the task in the specified environment and communicate to the Server the node status. CLI Command Line Interface to interact with the cluster, to manage and publish the artefacts. It's a mere wrapper over the REST API. See the CLI doucumentation for an overview . Replicant Operator that with the usage of a control repository (e.g. hosted on Github) deploys and maintain the infrastructure state Source . Bridge Standalone/Library suitable for creating custom hooks to listen at infrastructure events (e.g. IRC notifications when a new task is created, operators, etc. ) Tasks, Pipelines and Plans Tasks are the core concept of Mottainai . They define an environment where to execute a set of command, and they can produce content that is meant to be re-distributed later. Tasks are executed seamelessy in different scenarios, from containarized to virtualized environment, holding the same properties. Storages, Artefacts, Namespaces In Mottainai, every task can produce a set of artefacts , but also access as well to buckets that are needed during the task execution. That means that a task can re-use the content produced by other tasks, and actually track and manage their lifecycle. Artefacts can be later collected into so-called namespaces . Namespaces can be configured for fine-grained access (public, organization, private, internal, feature still in development ) and collect artefacts. The cli allows to control them, e.g. to tag their content, which allows fast rollbacks and cheap update channels. Persistent caching and image cache registries It's possible, only if the backend supports it, to have persistent image caches across the cluster by setting up a private registry (e.g. Docker private registries) or by using even dockerhub itself. When this feature is enabled per-task, the build environment will be carried over between different clusters, incrementally updating itself. Artefacts maintenance Tag, remove, download, upload and publish artefacts from your task within the same CLI or via REST API - you can even configure your infrastructure to deliver the data in different medium ( rsync, git, etc. ) but you can still manage the content from the same interface.","title":"How it works"},{"location":"general/#general-concepts","text":"Mottainai is composed of 4 main components: Mottainai Server - central node to dispatch tasks and collect artefacts from build Mottainai CLI - Application to interface with Mottainai Server Replicant - CLI Application to manage an infrastructure state Mottainai Agent - Runtime for nodes that are executing tasks Mottainai Bridge - Suite of library/binaries to listen on events emitted by the infrastructure","title":"General concepts"},{"location":"general/#server","text":"The Server is the main compontent which orchestrate builds across nodes: it dispatches tasks to the nodes using a broker implementation and it take cares of handling the artefacts from the build ( any kind of produced content from your build ). You can customize more the infrastructure behind it, and setup Ceph or other storage engines for permanent and distributed storage.","title":"Server"},{"location":"general/#agent","text":"The Agent is the main software which is executing the tasks, is being run by nodes belonging to the cluster. Among its duties, it takes care of executing the task in the specified environment and communicate to the Server the node status.","title":"Agent"},{"location":"general/#cli","text":"Command Line Interface to interact with the cluster, to manage and publish the artefacts. It's a mere wrapper over the REST API. See the CLI doucumentation for an overview .","title":"CLI"},{"location":"general/#replicant","text":"Operator that with the usage of a control repository (e.g. hosted on Github) deploys and maintain the infrastructure state Source .","title":"Replicant"},{"location":"general/#bridge","text":"Standalone/Library suitable for creating custom hooks to listen at infrastructure events (e.g. IRC notifications when a new task is created, operators, etc. )","title":"Bridge"},{"location":"general/#tasks-pipelines-and-plans","text":"Tasks are the core concept of Mottainai . They define an environment where to execute a set of command, and they can produce content that is meant to be re-distributed later. Tasks are executed seamelessy in different scenarios, from containarized to virtualized environment, holding the same properties.","title":"Tasks, Pipelines and Plans"},{"location":"general/#storages-artefacts-namespaces","text":"In Mottainai, every task can produce a set of artefacts , but also access as well to buckets that are needed during the task execution. That means that a task can re-use the content produced by other tasks, and actually track and manage their lifecycle. Artefacts can be later collected into so-called namespaces . Namespaces can be configured for fine-grained access (public, organization, private, internal, feature still in development ) and collect artefacts. The cli allows to control them, e.g. to tag their content, which allows fast rollbacks and cheap update channels.","title":"Storages, Artefacts, Namespaces"},{"location":"general/#persistent-caching-and-image-cache-registries","text":"It's possible, only if the backend supports it, to have persistent image caches across the cluster by setting up a private registry (e.g. Docker private registries) or by using even dockerhub itself. When this feature is enabled per-task, the build environment will be carried over between different clusters, incrementally updating itself.","title":"Persistent caching and image cache registries"},{"location":"general/#artefacts-maintenance","text":"Tag, remove, download, upload and publish artefacts from your task within the same CLI or via REST API - you can even configure your infrastructure to deliver the data in different medium ( rsync, git, etc. ) but you can still manage the content from the same interface.","title":"Artefacts maintenance"},{"location":"setup/","text":"Setup Here you an find how-to's for supported installation environment. Mottainai can be deployed natively with a package for your Linux distribution (currently only Gentoo/Sabayon is supported), used as static binaries, or deployed with docker-compose and kubernetes if you like. Server install Docker compose Kubernetes (WIP) Static binaries (WIP) Gentoo (WIP) Sabayon (WIP) Agent install Ephemeral agents Static binaries (WIP) Gentoo (WIP) Sabayon (WIP)","title":"Setup"},{"location":"setup/#setup","text":"Here you an find how-to's for supported installation environment. Mottainai can be deployed natively with a package for your Linux distribution (currently only Gentoo/Sabayon is supported), used as static binaries, or deployed with docker-compose and kubernetes if you like.","title":"Setup"},{"location":"setup/#server-install","text":"Docker compose Kubernetes (WIP) Static binaries (WIP) Gentoo (WIP) Sabayon (WIP)","title":"Server install"},{"location":"setup/#agent-install","text":"Ephemeral agents Static binaries (WIP) Gentoo (WIP) Sabayon (WIP)","title":"Agent install"},{"location":"setup/agent-ephemeral/","text":"Ephemeral agents Mottainai will guide you in the WebUI to create ephemeral agents to connect to your instance. Step 1 - Create a new Node In the Web Interface, go over the Node section, and hit the Create button (this can be performed also over CLI). You will see a new node in the table. Click on it Step 2 - Run the agent in the docker container Go on the node page (click on the Node id in the webui, that we just created before). You will see a tooltip which is giving you a docker command, you can copy-paste that command and run it in a machine with docker. That's it! Note : If your instance is created with Docker-compose, you need to replace all the redis (host) occurrence that you see in the command with your remote machine IP (e.g. if you are in a LAN with ip 192.168.1.2, you have to replace redis://redis:6379/2 with redis://192.168.1.2:6379/2 manually)","title":"Ephemeral agents"},{"location":"setup/agent-ephemeral/#ephemeral-agents","text":"Mottainai will guide you in the WebUI to create ephemeral agents to connect to your instance.","title":"Ephemeral agents"},{"location":"setup/agent-ephemeral/#step-1-create-a-new-node","text":"In the Web Interface, go over the Node section, and hit the Create button (this can be performed also over CLI). You will see a new node in the table. Click on it","title":"Step 1 - Create a new Node"},{"location":"setup/agent-ephemeral/#step-2-run-the-agent-in-the-docker-container","text":"Go on the node page (click on the Node id in the webui, that we just created before). You will see a tooltip which is giving you a docker command, you can copy-paste that command and run it in a machine with docker. That's it! Note : If your instance is created with Docker-compose, you need to replace all the redis (host) occurrence that you see in the command with your remote machine IP (e.g. if you are in a LAN with ip 192.168.1.2, you have to replace redis://redis:6379/2 with redis://192.168.1.2:6379/2 manually)","title":"Step 2 - Run the agent in the docker container"},{"location":"setup/dockercompose/","text":"Docker compose With Docker Compose, you will be able to start a ready-to-go instance in 3 steps! The Docker Compose stack is using Redis as a broker to dispatch messages to agent. Step 1 - Start docker (prerequisite) Make sure your docker daemon is running locally: $ systemctl is-active docker || sudo systemctl start docker Step 2 - Install Docker-compose (prerequisite) Make sure you have docker-compose installed, if is not available as a package for your Linux distribution, check the official docs , or you can: $ sudo curl -L https://github.com/docker/compose/releases/download/1.24.0/docker-compose- $( uname -s ) - $( uname -m ) -o /usr/local/bin/docker-compose $ sudo chmod +x /usr/local/bin/docker-compose Step 3 - Start the server # Clone the repo $ git clone https://github.com/MottainaiCI/mottainai-server # The docker-compose file is under contrib/docker-compose $ cd mottainai-server/contrib/docker-compose # Start the docker-compose stack. Drop -d if you want to run it in the foreground $ docker-compose up -d When the initialization sequence is completed, you are good to go, you can browse http://127.0.0.1:4545/ to see your instance. Register an account (the first registered is automatically given admin rights) and check on how to run an ephemeral agent .","title":"Docker compose"},{"location":"setup/dockercompose/#docker-compose","text":"With Docker Compose, you will be able to start a ready-to-go instance in 3 steps! The Docker Compose stack is using Redis as a broker to dispatch messages to agent.","title":"Docker compose"},{"location":"setup/dockercompose/#step-1-start-docker-prerequisite","text":"Make sure your docker daemon is running locally: $ systemctl is-active docker || sudo systemctl start docker","title":"Step 1 - Start docker (prerequisite)"},{"location":"setup/dockercompose/#step-2-install-docker-compose-prerequisite","text":"Make sure you have docker-compose installed, if is not available as a package for your Linux distribution, check the official docs , or you can: $ sudo curl -L https://github.com/docker/compose/releases/download/1.24.0/docker-compose- $( uname -s ) - $( uname -m ) -o /usr/local/bin/docker-compose $ sudo chmod +x /usr/local/bin/docker-compose","title":"Step 2 - Install Docker-compose (prerequisite)"},{"location":"setup/dockercompose/#step-3-start-the-server","text":"# Clone the repo $ git clone https://github.com/MottainaiCI/mottainai-server # The docker-compose file is under contrib/docker-compose $ cd mottainai-server/contrib/docker-compose # Start the docker-compose stack. Drop -d if you want to run it in the foreground $ docker-compose up -d When the initialization sequence is completed, you are good to go, you can browse http://127.0.0.1:4545/ to see your instance. Register an account (the first registered is automatically given admin rights) and check on how to run an ephemeral agent .","title":"Step 3 - Start the server"},{"location":"usage/artefacts/","text":"Artefacts In Mottainai, tasks can optionally produce Artefacts that you can later reference and tag into specific namespaces ( or here referred also as bucket ). In a task definition, you can specify to tag automatically on success into a namespace by setting tag_namespace : # Create an empty file as artefact script : - touch artefacts/hello.txt tag_namespace : my-bucket-name You can also map the artefact folder to a custom one, by defining artefact_path . artefact_path : out And use it like this in our script definition: script : - touch out/hello.txt artefact_path : out tag_namespace : my-bucket-name Append file to the bucket By 'tagging' a namespace automatically with the task, we are replacing the bucket content with the task artefacts, but this is not always the case. We can define the way we publish artefacts to the bucket specifying publish_mode in the task definition, e.g. to append file to the bucket just set it to append : # Specify artefacts publishing mode on namespaces. by default it replace namespace content during tag. # - append: Do not replace namespace content, append artefacts to existing ones publish_mode : append Get data from a bucket To retrieve data during our task execution instead, specify namespace in the task definition: script : - ls artefacts/hello.* namespace : some-bucket In this way, we can retrieve the same data belonging to the bucket, allowing to pass-by the content between different tasks, even from different task types. Manually tag artefacts Using the CLI it's possible to tag tasks artefacts into new namespaces and also merging and replacing their content. Use the task id which you want to use as a source for the namespace content, e.g. mottainai-cli namespace tag some-bucket --from 123456 Will tag the artefacts produced by 123456 into the some-bucket namespace. Append task artefacts to a namespace In the same way as tagging, it's possible to append tasks artefacts into existing namespaces. Use the task id which you want to use as a source for the namespace content, e.g. mottainai-cli namespace append some-bucket --from 123456 Will append the artefacts produced by 123456 into the some-bucket namespace.","title":"Artefacts"},{"location":"usage/artefacts/#artefacts","text":"In Mottainai, tasks can optionally produce Artefacts that you can later reference and tag into specific namespaces ( or here referred also as bucket ). In a task definition, you can specify to tag automatically on success into a namespace by setting tag_namespace : # Create an empty file as artefact script : - touch artefacts/hello.txt tag_namespace : my-bucket-name You can also map the artefact folder to a custom one, by defining artefact_path . artefact_path : out And use it like this in our script definition: script : - touch out/hello.txt artefact_path : out tag_namespace : my-bucket-name","title":"Artefacts"},{"location":"usage/artefacts/#append-file-to-the-bucket","text":"By 'tagging' a namespace automatically with the task, we are replacing the bucket content with the task artefacts, but this is not always the case. We can define the way we publish artefacts to the bucket specifying publish_mode in the task definition, e.g. to append file to the bucket just set it to append : # Specify artefacts publishing mode on namespaces. by default it replace namespace content during tag. # - append: Do not replace namespace content, append artefacts to existing ones publish_mode : append","title":"Append file to the bucket"},{"location":"usage/artefacts/#get-data-from-a-bucket","text":"To retrieve data during our task execution instead, specify namespace in the task definition: script : - ls artefacts/hello.* namespace : some-bucket In this way, we can retrieve the same data belonging to the bucket, allowing to pass-by the content between different tasks, even from different task types.","title":"Get data from a bucket"},{"location":"usage/artefacts/#manually-tag-artefacts","text":"Using the CLI it's possible to tag tasks artefacts into new namespaces and also merging and replacing their content. Use the task id which you want to use as a source for the namespace content, e.g. mottainai-cli namespace tag some-bucket --from 123456 Will tag the artefacts produced by 123456 into the some-bucket namespace.","title":"Manually tag artefacts"},{"location":"usage/artefacts/#append-task-artefacts-to-a-namespace","text":"In the same way as tagging, it's possible to append tasks artefacts into existing namespaces. Use the task id which you want to use as a source for the namespace content, e.g. mottainai-cli namespace append some-bucket --from 123456 Will append the artefacts produced by 123456 into the some-bucket namespace.","title":"Append task artefacts to a namespace"},{"location":"usage/cli/","text":"Command Line Interface You can use the command line interface to create, track and manage your cluster. Mottainai CLI Copyright (c) 2017-2018 Mottainai Command line interface for Mottainai clusters Usage: [flags] [command] Examples: $ mottainai-cli -m http://127.0.0.1:8080 task create --json task.json $ mottainai-cli -m http://127.0.0.1:8080 namespace list Available Commands: help Help about any command namespace Manage namespaces node Manage nodes pipeline Manage Pipeline of Task plan Manage Planning of Task profile Manage CLI Profiles setting Manage Infrastructure settings simulate Simulate Agent Command storage Manage storages task Manage tasks token Manage tokens user Manage users Flags: -k, --apikey string Mottainai API key (default fb4h3bhgv4421355 ) -h, --help help for this command -m, --master string MottainaiCI webUI URL (default http://localhost:8080 ) -p, --profile string Use specific profile for call API. Use [command] --help for more information about a command. Authentication Note: The CLI options --master and --apikey are required for every command issued to the CLI, see how to set CLI profiles to store credentials in your machine to avoid typing them each time. In the following commands will be omitted for sake of readability, but they are required. Tasks Manage tasks Usage: task [command] Available Commands: artefacts Show artefacts of a task attach attach to a task output clone clone a task create Create a new task download Download task artefacts execute execute task list List tasks log Show log of a task monitor Monitor tasks and propagate exit status remove Remove a task show Show a task start Start a task stop Stop a task Flags: -h, --help help for task Global Flags: -k, --apikey string Mottainai API key (default fb4h3bhgv4421355 ) -m, --master string MottainaiCI webUI URL (default http://localhost:8080 ) -p, --profile string Use specific profile for call API. Use task [command] --help for more information about a command. Create After defining a task inside a yaml or json file you can use the CLI to create tasks and pipelines to be executed remotely by the nodes belonging to your cluster. $ mottainai-cli task create --yaml file.yaml Attach Once the task is created, the CLI will return an ID and instructions on how to interact with it. If you want to see the logs of a running task and see updates, run: $ mottainai-cli task attach #taskid Stop To stop a running task: $ mottainai-cli task stop #taskid Remove To remove a task, and its associated artefacts: $ mottainai-cli task remove #taskid Monitor Wait for tasks to complete, and propagate error to the console : $ mottainai-cli task monitor #taskid #taskid2 #taskid3 ... Log Get task current log: $ mottainai-cli task log #taskid Download You can download a task artefacts with the CLI: mottainai-cli task download #taskid /my/download/folder List List all tasks: mottainai-cli task list Clone Generate a new task from an old one: mottainai-cli task clone #taskid Profiles Manage CLI Profiles Usage: profile [command] Available Commands: create Create a new profile list List profiles remove Remove a profile Flags: -h, --help help for profile Global Flags: -k, --apikey string Mottainai API key (default fb4h3bhgv4421355 ) -m, --master string MottainaiCI webUI URL (default http://localhost:8080 ) -p, --profile string Use specific profile for call API. Use profile [command] --help for more information about a command. Profiles can be used to store permanently options that CLI uses to connect and authenticate to the Mottainai Server. To create a new profile: $ mottainai-cli profile create #profilename #APIurl #APIkey Later, when you are using the cli you can specify $ mottainai-cli --profile #profilename commmands... See mottainai-cli profile --help for an overview. To define a default profile, you can set the environment variable MOTTAINAI_CLI_PROFILE inside your .bashrc (or equivalent), in this way you can issue command directly, for example: $ mottainai-cli task attach #taskid Namespace Manage namespaces Usage: namespace [command] Available Commands: clone clone a namespace create Create a namespace delete Delete a namespace download Download namespace artefacts list List namespaces show Show artefacts belonging to namespace tag Tag a namespace upload Upload file to a namespace Flags: -h, --help help for namespace Global Flags: -k, --apikey string Mottainai API key (default fb4h3bhgv4421355 ) -m, --master string MottainaiCI webUI URL (default http://localhost:8080 ) -p, --profile string Use specific profile for call API. Use namespace [command] --help for more information about a command. Create Create a namespace: $ mottainai-cli namespace create #name To default, normal users have access only to namespaces which prefix with their username. Administrators or Managers aren't subject to such restriction. Download Download all files in a namespace $ mottainai-cli namespace download #name /my/download/path Upload Upload a file in a namespace, note destination is in absolute form in respect of the namespace content (e.g. / to push to the root of namespace): $ mottainai-cli namespace upload #namespace #file subfolder/ Delete Delete namespace (permanently): $ mottainai-cli namespace delete #name List List all namespaces: $ mottainai-cli namespace list Clone Generate a new namespace from an old one: $ mottainai-cli namespace clone #name #newname Tag Publish task produced artefacts into a namespace: $ mottainai-cli namespace tag #name --from #taskid Storage Manage storages Usage: storage [command] Available Commands: create Create a storage delete Delete a storage download Download storage artefacts list List storages show Show artefacts belonging to a storage upload Upload file to a storage Flags: -h, --help help for storage Global Flags: -k, --apikey string Mottainai API key (default fb4h3bhgv4421355 ) -m, --master string MottainaiCI webUI URL (default http://localhost:8080 ) -p, --profile string Use specific profile for call API. Use storage [command] --help for more information about a command. Create Create a storage: $ mottainai-cli storage create #name To default, normal users have access only to namespaces which prefix with their username (e.g. user::foo ). Administrators or Managers aren't subject to such restriction. Download Download all files in a storage $ mottainai-cli storage download #storageid /my/download/path Upload Upload a file in a storage, note destination is in absolute form in respect of the storage content (e.g. / to push to the root of namespace): $ mottainai-cli storage upload #storageid #file subpath/ Delete Delete storage (permanently): $ mottainai-cli storage delete #storageid List List all storages: $ mottainai-cli storage list","title":"Cli"},{"location":"usage/cli/#command-line-interface","text":"You can use the command line interface to create, track and manage your cluster. Mottainai CLI Copyright (c) 2017-2018 Mottainai Command line interface for Mottainai clusters Usage: [flags] [command] Examples: $ mottainai-cli -m http://127.0.0.1:8080 task create --json task.json $ mottainai-cli -m http://127.0.0.1:8080 namespace list Available Commands: help Help about any command namespace Manage namespaces node Manage nodes pipeline Manage Pipeline of Task plan Manage Planning of Task profile Manage CLI Profiles setting Manage Infrastructure settings simulate Simulate Agent Command storage Manage storages task Manage tasks token Manage tokens user Manage users Flags: -k, --apikey string Mottainai API key (default fb4h3bhgv4421355 ) -h, --help help for this command -m, --master string MottainaiCI webUI URL (default http://localhost:8080 ) -p, --profile string Use specific profile for call API. Use [command] --help for more information about a command.","title":"Command Line Interface"},{"location":"usage/cli/#authentication","text":"Note: The CLI options --master and --apikey are required for every command issued to the CLI, see how to set CLI profiles to store credentials in your machine to avoid typing them each time. In the following commands will be omitted for sake of readability, but they are required.","title":"Authentication"},{"location":"usage/cli/#tasks","text":"Manage tasks Usage: task [command] Available Commands: artefacts Show artefacts of a task attach attach to a task output clone clone a task create Create a new task download Download task artefacts execute execute task list List tasks log Show log of a task monitor Monitor tasks and propagate exit status remove Remove a task show Show a task start Start a task stop Stop a task Flags: -h, --help help for task Global Flags: -k, --apikey string Mottainai API key (default fb4h3bhgv4421355 ) -m, --master string MottainaiCI webUI URL (default http://localhost:8080 ) -p, --profile string Use specific profile for call API. Use task [command] --help for more information about a command.","title":"Tasks"},{"location":"usage/cli/#create","text":"After defining a task inside a yaml or json file you can use the CLI to create tasks and pipelines to be executed remotely by the nodes belonging to your cluster. $ mottainai-cli task create --yaml file.yaml","title":"Create"},{"location":"usage/cli/#attach","text":"Once the task is created, the CLI will return an ID and instructions on how to interact with it. If you want to see the logs of a running task and see updates, run: $ mottainai-cli task attach #taskid","title":"Attach"},{"location":"usage/cli/#stop","text":"To stop a running task: $ mottainai-cli task stop #taskid","title":"Stop"},{"location":"usage/cli/#remove","text":"To remove a task, and its associated artefacts: $ mottainai-cli task remove #taskid","title":"Remove"},{"location":"usage/cli/#monitor","text":"Wait for tasks to complete, and propagate error to the console : $ mottainai-cli task monitor #taskid #taskid2 #taskid3 ...","title":"Monitor"},{"location":"usage/cli/#log","text":"Get task current log: $ mottainai-cli task log #taskid","title":"Log"},{"location":"usage/cli/#download","text":"You can download a task artefacts with the CLI: mottainai-cli task download #taskid /my/download/folder","title":"Download"},{"location":"usage/cli/#list","text":"List all tasks: mottainai-cli task list","title":"List"},{"location":"usage/cli/#clone","text":"Generate a new task from an old one: mottainai-cli task clone #taskid","title":"Clone"},{"location":"usage/cli/#profiles","text":"Manage CLI Profiles Usage: profile [command] Available Commands: create Create a new profile list List profiles remove Remove a profile Flags: -h, --help help for profile Global Flags: -k, --apikey string Mottainai API key (default fb4h3bhgv4421355 ) -m, --master string MottainaiCI webUI URL (default http://localhost:8080 ) -p, --profile string Use specific profile for call API. Use profile [command] --help for more information about a command. Profiles can be used to store permanently options that CLI uses to connect and authenticate to the Mottainai Server. To create a new profile: $ mottainai-cli profile create #profilename #APIurl #APIkey Later, when you are using the cli you can specify $ mottainai-cli --profile #profilename commmands... See mottainai-cli profile --help for an overview. To define a default profile, you can set the environment variable MOTTAINAI_CLI_PROFILE inside your .bashrc (or equivalent), in this way you can issue command directly, for example: $ mottainai-cli task attach #taskid","title":"Profiles"},{"location":"usage/cli/#namespace","text":"Manage namespaces Usage: namespace [command] Available Commands: clone clone a namespace create Create a namespace delete Delete a namespace download Download namespace artefacts list List namespaces show Show artefacts belonging to namespace tag Tag a namespace upload Upload file to a namespace Flags: -h, --help help for namespace Global Flags: -k, --apikey string Mottainai API key (default fb4h3bhgv4421355 ) -m, --master string MottainaiCI webUI URL (default http://localhost:8080 ) -p, --profile string Use specific profile for call API. Use namespace [command] --help for more information about a command.","title":"Namespace"},{"location":"usage/cli/#create_1","text":"Create a namespace: $ mottainai-cli namespace create #name To default, normal users have access only to namespaces which prefix with their username. Administrators or Managers aren't subject to such restriction.","title":"Create"},{"location":"usage/cli/#download_1","text":"Download all files in a namespace $ mottainai-cli namespace download #name /my/download/path","title":"Download"},{"location":"usage/cli/#upload","text":"Upload a file in a namespace, note destination is in absolute form in respect of the namespace content (e.g. / to push to the root of namespace): $ mottainai-cli namespace upload #namespace #file subfolder/","title":"Upload"},{"location":"usage/cli/#delete","text":"Delete namespace (permanently): $ mottainai-cli namespace delete #name","title":"Delete"},{"location":"usage/cli/#list_1","text":"List all namespaces: $ mottainai-cli namespace list","title":"List"},{"location":"usage/cli/#clone_1","text":"Generate a new namespace from an old one: $ mottainai-cli namespace clone #name #newname","title":"Clone"},{"location":"usage/cli/#tag","text":"Publish task produced artefacts into a namespace: $ mottainai-cli namespace tag #name --from #taskid","title":"Tag"},{"location":"usage/cli/#storage","text":"Manage storages Usage: storage [command] Available Commands: create Create a storage delete Delete a storage download Download storage artefacts list List storages show Show artefacts belonging to a storage upload Upload file to a storage Flags: -h, --help help for storage Global Flags: -k, --apikey string Mottainai API key (default fb4h3bhgv4421355 ) -m, --master string MottainaiCI webUI URL (default http://localhost:8080 ) -p, --profile string Use specific profile for call API. Use storage [command] --help for more information about a command.","title":"Storage"},{"location":"usage/cli/#create_2","text":"Create a storage: $ mottainai-cli storage create #name To default, normal users have access only to namespaces which prefix with their username (e.g. user::foo ). Administrators or Managers aren't subject to such restriction.","title":"Create"},{"location":"usage/cli/#download_2","text":"Download all files in a storage $ mottainai-cli storage download #storageid /my/download/path","title":"Download"},{"location":"usage/cli/#upload_1","text":"Upload a file in a storage, note destination is in absolute form in respect of the storage content (e.g. / to push to the root of namespace): $ mottainai-cli storage upload #storageid #file subpath/","title":"Upload"},{"location":"usage/cli/#delete_1","text":"Delete storage (permanently): $ mottainai-cli storage delete #storageid","title":"Delete"},{"location":"usage/cli/#list_2","text":"List all storages: $ mottainai-cli storage list","title":"List"},{"location":"usage/namespace/","text":"Namespace Namespaces allows you to publish and manage content that is subject to a release process. Artefacts produced from Tasks and Pipelines can be organized into namespaces. Namespaces are accessible from the web interface: https : //your.instance.com/namespace/[namespace name] Only administrators are allowed (for now) to publish in top layer ones, standard users have access only to namespaces which prefixes with their username. Create You can create a new namespace from the CLI with: $ mottainai-cli namespace create new-namespace On the task (or pipeline) definition you can publish the resulting artefact to the namespace by adding tag_namespace: new-namespace . Clone You can create a new namespace cloning from an old one: $ mottainai-cli namespace clone --from old-namespace new-namespace Delete You can delete a namespace from the CLI with: $ mottainai-cli namespace delete some-namespace Remove You can remove files from a namespace from the CLI with: $ mottainai-cli namespace remove some-namespace /path/to/file e.g. if the foobar namespace have a file at the top level called 'hello.txt', you can remove it with: $ mottainai-cli namespace remove foobar /hello.txt","title":"Namespaces"},{"location":"usage/namespace/#namespace","text":"Namespaces allows you to publish and manage content that is subject to a release process. Artefacts produced from Tasks and Pipelines can be organized into namespaces. Namespaces are accessible from the web interface: https : //your.instance.com/namespace/[namespace name] Only administrators are allowed (for now) to publish in top layer ones, standard users have access only to namespaces which prefixes with their username.","title":"Namespace"},{"location":"usage/namespace/#create","text":"You can create a new namespace from the CLI with: $ mottainai-cli namespace create new-namespace On the task (or pipeline) definition you can publish the resulting artefact to the namespace by adding tag_namespace: new-namespace .","title":"Create"},{"location":"usage/namespace/#clone","text":"You can create a new namespace cloning from an old one: $ mottainai-cli namespace clone --from old-namespace new-namespace","title":"Clone"},{"location":"usage/namespace/#delete","text":"You can delete a namespace from the CLI with: $ mottainai-cli namespace delete some-namespace","title":"Delete"},{"location":"usage/namespace/#remove","text":"You can remove files from a namespace from the CLI with: $ mottainai-cli namespace remove some-namespace /path/to/file e.g. if the foobar namespace have a file at the top level called 'hello.txt', you can remove it with: $ mottainai-cli namespace remove foobar /hello.txt","title":"Remove"},{"location":"usage/plans/","text":"Plans Plans in Mottainai are special tasks with an extra field, planned , which is indicating the recurrence of a task. This means they heredit the full syntax from the tasks, the only difference stands for the extra field ( \" planned \" ). Plans can be submitted to the infrastructure with the CLI , or with Replicant . Syntax Full task example # Task name name : My task # Image used by the task player image : my/image # Script to be executed script : - echo hello world # Task type type : docker # We want the task to be executed weekly planned : @weekly planned fully supports the cron syntax, here you can find a short summary. Note : Mottainai it's using internally robfig/cron , this is an extract from their documentation for reference. Head over here for a full explanation of the supported syntax . Predefined schedules Entry Description Equivalent To @yearly (or @annually) Run once a year, midnight, Jan. 1st 0 0 0 1 1 * @monthly Run once a month, midnight, first of month 0 0 0 1 * * @weekly Run once a week, midnight between Sat/Sun 0 0 0 * * 0 @daily (or @midnight) Run once a day, midnight 0 0 0 * * * @hourly Run once an hour, beginning of hour 0 0 * * Intervals You may also schedule a job to execute at fixed intervals, starting at the time it's added. This is supported by using the planned field like this: @every duration where \"duration\" is a string accepted by time.ParseDuration (http://golang.org/pkg/time/#ParseDuration). For example, \" @every 1h30m10s \" would indicate a schedule that activates after 1 hour, 30 minutes, 10 seconds, and then every interval after that. Note : The interval does not take the job runtime into account. For example, if a job takes 3 minutes to run, and it is scheduled to run every 5 minutes, it will have only 2 minutes of idle time between each run. Replicant To deploy all plans defined in a Git repository, you can use Replicant . In order to keep trace of the plans in the infrastructure, Replicant parses all plans in a Git repository, and automatically submits and keep the infrastructure in sync with the content. You can even combine and run a Replicant task which is a webhook inside Mottainai, that automatically keeps your infrastructure updated.","title":"Plans"},{"location":"usage/plans/#plans","text":"Plans in Mottainai are special tasks with an extra field, planned , which is indicating the recurrence of a task. This means they heredit the full syntax from the tasks, the only difference stands for the extra field ( \" planned \" ). Plans can be submitted to the infrastructure with the CLI , or with Replicant .","title":"Plans"},{"location":"usage/plans/#syntax","text":"Full task example # Task name name : My task # Image used by the task player image : my/image # Script to be executed script : - echo hello world # Task type type : docker # We want the task to be executed weekly planned : @weekly planned fully supports the cron syntax, here you can find a short summary. Note : Mottainai it's using internally robfig/cron , this is an extract from their documentation for reference. Head over here for a full explanation of the supported syntax .","title":"Syntax"},{"location":"usage/plans/#predefined-schedules","text":"Entry Description Equivalent To @yearly (or @annually) Run once a year, midnight, Jan. 1st 0 0 0 1 1 * @monthly Run once a month, midnight, first of month 0 0 0 1 * * @weekly Run once a week, midnight between Sat/Sun 0 0 0 * * 0 @daily (or @midnight) Run once a day, midnight 0 0 0 * * * @hourly Run once an hour, beginning of hour 0 0 * *","title":"Predefined schedules"},{"location":"usage/plans/#intervals","text":"You may also schedule a job to execute at fixed intervals, starting at the time it's added. This is supported by using the planned field like this: @every duration where \"duration\" is a string accepted by time.ParseDuration (http://golang.org/pkg/time/#ParseDuration). For example, \" @every 1h30m10s \" would indicate a schedule that activates after 1 hour, 30 minutes, 10 seconds, and then every interval after that. Note : The interval does not take the job runtime into account. For example, if a job takes 3 minutes to run, and it is scheduled to run every 5 minutes, it will have only 2 minutes of idle time between each run.","title":"Intervals"},{"location":"usage/plans/#replicant","text":"To deploy all plans defined in a Git repository, you can use Replicant . In order to keep trace of the plans in the infrastructure, Replicant parses all plans in a Git repository, and automatically submits and keep the infrastructure in sync with the content. You can even combine and run a Replicant task which is a webhook inside Mottainai, that automatically keeps your infrastructure updated.","title":"Replicant"},{"location":"usage/storage/","text":"Storage Storages are virtual buckets where it is possible to upload content from the CLI or web-interface, that can be retrieved from the task during the execution. Storages are referenced in the task fields by ID, and you can use them to e.g. provide separate files that cannot be shared in the git repositories and that are not subject to public access. Users can create storages which prefixes with their name, while administrators can create storages of any kind. To create a new storage, use the CLI: $ mottainai-cli storage create foo An ID is returned from the CLI, which you can later use in the tasks definitions to reference to the storage. $ mottainai-cli storage create test 3406173479640723926 In the task definition, annotate the ID in the storage field. Inside the task script now you can reference to the storage in the storage/ directory relative to your task execution, e.g.: script : - ls storage/ - source storage/.secrets.env - ... Storages are not uploaded from an agent node, they just are read-only for workers. Upload You can upload files from the cli to your storage. $ mottainai-cli storage upload #ID #file_to_upload #storage_path The #ID is the identifier of the storage (e.g. 3406173479640723926 ), #file_to_upload is the path of the file you wish to upload, and #storage_path is the absolute path relative to the storage where it should be moved to, e.g. if we have a hello_world.txt in the current dir: $ mottainai-cli storage upload 3406173479640723926 hello_world.txt /hello_world.txt In the task which is referencing it, hello_world.txt now is accessible: script : - ls storage/ - cat storage/hello_world.txt - ...","title":"Storages"},{"location":"usage/storage/#storage","text":"Storages are virtual buckets where it is possible to upload content from the CLI or web-interface, that can be retrieved from the task during the execution. Storages are referenced in the task fields by ID, and you can use them to e.g. provide separate files that cannot be shared in the git repositories and that are not subject to public access. Users can create storages which prefixes with their name, while administrators can create storages of any kind. To create a new storage, use the CLI: $ mottainai-cli storage create foo An ID is returned from the CLI, which you can later use in the tasks definitions to reference to the storage. $ mottainai-cli storage create test 3406173479640723926 In the task definition, annotate the ID in the storage field. Inside the task script now you can reference to the storage in the storage/ directory relative to your task execution, e.g.: script : - ls storage/ - source storage/.secrets.env - ... Storages are not uploaded from an agent node, they just are read-only for workers.","title":"Storage"},{"location":"usage/storage/#upload","text":"You can upload files from the cli to your storage. $ mottainai-cli storage upload #ID #file_to_upload #storage_path The #ID is the identifier of the storage (e.g. 3406173479640723926 ), #file_to_upload is the path of the file you wish to upload, and #storage_path is the absolute path relative to the storage where it should be moved to, e.g. if we have a hello_world.txt in the current dir: $ mottainai-cli storage upload 3406173479640723926 hello_world.txt /hello_world.txt In the task which is referencing it, hello_world.txt now is accessible: script : - ls storage/ - cat storage/hello_world.txt - ...","title":"Upload"},{"location":"usage/tasksandpipelines/","text":"Tasks Tasks can be defined in both json or yaml, it boils down to: # Task name name : My task # Image used by the task player image : my/image # Script to be executed script : - echo hello world # Task type type : docker Those are the required fields: image : it's the environment where your task is going to be run script : list of commands to execute type : it's the task type ( vagrant_virtualbox, vagrant_libvirt, docker, lxd ) Once you have your task definition done, create your task into the infrastructure with: $ mottainai-cli task create --yaml task.yaml If all goes as expected, you will see an url corresponding to your running task. There are more field to allow you to control better the lifespan of the task, or further define the environment. This is a full task definition: # Task name name : My task # Image used by the task player image : my/image # Script to be executed script : - echo hello world # Task type type : docker # Git repository that will be the context of our build # The node that will execute the task will fetch the content and # executes the commands provided source : https://github.com/user/repo # Path relative to the git repository in which the commands will be executed into directory : /dir/ # List of path to map inside the build environment from the host binds : - /dev/loop-control:/dev/loop-control - /test # Set to yes or any value to cache the image used to run the task, otherwise omit cache_image : yes # Set to yes or any value to wipe the cache for the task, otherwise omit cache_clean : yes # Custom image entrypoint ( first executed binary, defaults to /bin/bash ) entrypoint : - /bin/bash - -c # List of variables in the build environment environment : - FOO=42 # Delay task execution of 800s eta : 800 # Drain artefacts from the supplied namespace namespace : staging:development # Perform pruning commands after task execution (backend dependent), omit to not execute prune : yes # A specific queue where to send the task to. queue : amd64 # Task id where to drain artefacts from root_task : 34132345135151 # Task associated storage storage : 8925468933589065900 # relative path for drained storages (defaults storage) storage_path : storage # relative path for drained artefacts from namespace/root_task (defaults artefacts) artefact_path : artefacts # On success, tag task s produced artefacts into a namespace (create if doesn t exists) tag_namespace : staging:iso # Specify artefacts publishing mode on namespaces. by default it replace namespace content during tag. # - append: Do not replace namespace content, append to existing one publish_mode : append # Task time limit (in seconds) timeout : 0 # Retry attempts (in case of error) retry : 1 Most notably, all optional fields: source : Git repository where your source code is hosted. The node will fetch it and the task commands will be executed in the cloned repository directory : Define a directory inside your Git repository which will be where your sequence of commands will be executed queue : Specify a queue where to send the task to. If you setup your nodes to listen in different queues, you can exploit it to partition your cluster based on your preferences (e.g. architecture, host capabilities, etc. ) storage : Storage ID which the task has access to - depending on the user permissions. See Storage for more detail. namespace : Namespace is a unique string (as e.g. moo ) to drain artefacts from. Already published artefacts can be shared in such way between different tasks in different hosts. See Namespace for more detail. tag_namespace : Namespace is a unique string (as e.g. sheep ) where to tag task produced content automatically on success. See also Namespace and Artefacts for more detail. Pipelines In order to automate furthermore your infrastructure, it's possible to define pipelines of tasks. Tasks can be chained together to e.g. pass-by artifacts between each other, and if one of them fails, the chain is interrupted. There are 3 kinds of pipelines: Groups, Chains and Chords. Groups A group of task is a parallel group that is meant to run together. pipeline_name : My Pipeline concurrency : 1 # Number of parallel builds group : - task1 - task2 - task3 queue : general # Optional, force the pipeline to a specific queue tasks : task1 : image : sabayon/base-amd64 script : - echo hello type : docker task2 : image : sabayon/base-amd64 script : - echo hello 2! type : docker task3 : script : - echo hello 3 - exit 1 # Make it fail! type : docker tasks (required): A list of task definition in form key - task group (required): A list of task keys that are refering to the task definition pipeline_name (required): Name of the pipeline concurrency (optional): Number of parallel builds To run the pipeline, use the CLI as usual: $ mottainai-cli pipeline create --yaml task.yaml Chains Chains are sequences of task, each one is started only if the predecessor in the chain succeeded. pipeline_name : My Pipeline chain : - task - task2 - task3 tasks : test : image : sabayon/base-amd64 script : - echo hello type : docker task2 : image : sabayon/base-amd64 script : - echo hello 2! # - exit 1 If this fails, then task3 is not executed. type : docker task3 : image : sabayon/base-amd64 script : - echo hello from 3 type : docker As you can see the pipeline task field is a list of task that are indexed by a string. You can create keys freely and use them to define the execution order. Relevant fields not seen already: group (required): A list of task keys that are refering to the task definition Chords Execute a callback after a group ends successfully: pipeline_name : My Pipeline group : - task1 - task2 chord : - task3 tasks : task1 : image : sabayon/base-amd64 script : - echo hello type : docker task2 : image : sabayon/base-amd64 script : - echo hello world! # - exit 1 If it fails, task3 is not executed type : docker task3 : image : sabayon/base-amd64 script : - echo hello from 3 type : docker Relevant fields not seen already: chord : A list of task executed if chord is successfull (currently, just one callback is supported)","title":"Tasks and Pipelines"},{"location":"usage/tasksandpipelines/#tasks","text":"Tasks can be defined in both json or yaml, it boils down to: # Task name name : My task # Image used by the task player image : my/image # Script to be executed script : - echo hello world # Task type type : docker Those are the required fields: image : it's the environment where your task is going to be run script : list of commands to execute type : it's the task type ( vagrant_virtualbox, vagrant_libvirt, docker, lxd ) Once you have your task definition done, create your task into the infrastructure with: $ mottainai-cli task create --yaml task.yaml If all goes as expected, you will see an url corresponding to your running task. There are more field to allow you to control better the lifespan of the task, or further define the environment. This is a full task definition: # Task name name : My task # Image used by the task player image : my/image # Script to be executed script : - echo hello world # Task type type : docker # Git repository that will be the context of our build # The node that will execute the task will fetch the content and # executes the commands provided source : https://github.com/user/repo # Path relative to the git repository in which the commands will be executed into directory : /dir/ # List of path to map inside the build environment from the host binds : - /dev/loop-control:/dev/loop-control - /test # Set to yes or any value to cache the image used to run the task, otherwise omit cache_image : yes # Set to yes or any value to wipe the cache for the task, otherwise omit cache_clean : yes # Custom image entrypoint ( first executed binary, defaults to /bin/bash ) entrypoint : - /bin/bash - -c # List of variables in the build environment environment : - FOO=42 # Delay task execution of 800s eta : 800 # Drain artefacts from the supplied namespace namespace : staging:development # Perform pruning commands after task execution (backend dependent), omit to not execute prune : yes # A specific queue where to send the task to. queue : amd64 # Task id where to drain artefacts from root_task : 34132345135151 # Task associated storage storage : 8925468933589065900 # relative path for drained storages (defaults storage) storage_path : storage # relative path for drained artefacts from namespace/root_task (defaults artefacts) artefact_path : artefacts # On success, tag task s produced artefacts into a namespace (create if doesn t exists) tag_namespace : staging:iso # Specify artefacts publishing mode on namespaces. by default it replace namespace content during tag. # - append: Do not replace namespace content, append to existing one publish_mode : append # Task time limit (in seconds) timeout : 0 # Retry attempts (in case of error) retry : 1 Most notably, all optional fields: source : Git repository where your source code is hosted. The node will fetch it and the task commands will be executed in the cloned repository directory : Define a directory inside your Git repository which will be where your sequence of commands will be executed queue : Specify a queue where to send the task to. If you setup your nodes to listen in different queues, you can exploit it to partition your cluster based on your preferences (e.g. architecture, host capabilities, etc. ) storage : Storage ID which the task has access to - depending on the user permissions. See Storage for more detail. namespace : Namespace is a unique string (as e.g. moo ) to drain artefacts from. Already published artefacts can be shared in such way between different tasks in different hosts. See Namespace for more detail. tag_namespace : Namespace is a unique string (as e.g. sheep ) where to tag task produced content automatically on success. See also Namespace and Artefacts for more detail.","title":"Tasks"},{"location":"usage/tasksandpipelines/#pipelines","text":"In order to automate furthermore your infrastructure, it's possible to define pipelines of tasks. Tasks can be chained together to e.g. pass-by artifacts between each other, and if one of them fails, the chain is interrupted. There are 3 kinds of pipelines: Groups, Chains and Chords.","title":"Pipelines"},{"location":"usage/tasksandpipelines/#groups","text":"A group of task is a parallel group that is meant to run together. pipeline_name : My Pipeline concurrency : 1 # Number of parallel builds group : - task1 - task2 - task3 queue : general # Optional, force the pipeline to a specific queue tasks : task1 : image : sabayon/base-amd64 script : - echo hello type : docker task2 : image : sabayon/base-amd64 script : - echo hello 2! type : docker task3 : script : - echo hello 3 - exit 1 # Make it fail! type : docker tasks (required): A list of task definition in form key - task group (required): A list of task keys that are refering to the task definition pipeline_name (required): Name of the pipeline concurrency (optional): Number of parallel builds To run the pipeline, use the CLI as usual: $ mottainai-cli pipeline create --yaml task.yaml","title":"Groups"},{"location":"usage/tasksandpipelines/#chains","text":"Chains are sequences of task, each one is started only if the predecessor in the chain succeeded. pipeline_name : My Pipeline chain : - task - task2 - task3 tasks : test : image : sabayon/base-amd64 script : - echo hello type : docker task2 : image : sabayon/base-amd64 script : - echo hello 2! # - exit 1 If this fails, then task3 is not executed. type : docker task3 : image : sabayon/base-amd64 script : - echo hello from 3 type : docker As you can see the pipeline task field is a list of task that are indexed by a string. You can create keys freely and use them to define the execution order. Relevant fields not seen already: group (required): A list of task keys that are refering to the task definition","title":"Chains"},{"location":"usage/tasksandpipelines/#chords","text":"Execute a callback after a group ends successfully: pipeline_name : My Pipeline group : - task1 - task2 chord : - task3 tasks : task1 : image : sabayon/base-amd64 script : - echo hello type : docker task2 : image : sabayon/base-amd64 script : - echo hello world! # - exit 1 If it fails, task3 is not executed type : docker task3 : image : sabayon/base-amd64 script : - echo hello from 3 type : docker Relevant fields not seen already: chord : A list of task executed if chord is successfull (currently, just one callback is supported)","title":"Chords"}]}